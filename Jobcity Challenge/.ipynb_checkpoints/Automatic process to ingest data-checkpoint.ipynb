{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://handluggageonly.co.uk/wp-content/uploads/2017/03/Europe-Map-clipart.jpg width=300 align=right>\n",
    "\n",
    "\n",
    "<h1><left>Jobcity Challenge </left></h1>\n",
    "<h4><left>Author: Felipe Gibert </left></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  INSTRUCTIONS\n",
    "\n",
    "There must be an automated process to ingest and store the data.\n",
    "\n",
    "● Trips with similar origin, destination, and time of day should be grouped together.\n",
    "\n",
    "● Develop a way to obtain the weekly average number of trips for an area, defined by a\n",
    "bounding box (given by coordinates) or by a region.\n",
    "\n",
    "● Develop a way to inform the user about the status of the data ingestion without using a\n",
    "polling solution.\n",
    "\n",
    "● The solution should be scalable to 100 million entries. It is encouraged to simplify the\n",
    "data by a data model. Please add proof that the solution is scalable.\n",
    "\n",
    "● Use a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions:\n",
    "\n",
    "- A csv file or a xlsx file will be inserted into a \"analyze_folder\" and then be extracted to be load in a SQL server\n",
    "\n",
    "- The files will have the same format and column names that the example in the \"data\" folder\n",
    "\n",
    "- In case the file is inserted in the server generating duplicates. It would be solve in a different solution\n",
    "\n",
    "- The used path for the \"analyze_folder\" and the direction fro the SQL Server are using local references just for the purpouse of this exercise.\n",
    "\n",
    "- The way used to schedule this process is through crontab. Include in this github you will find the code to use in crontab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from datetime import datetime,timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL Connection\n",
    "\n",
    "import pyodbc\n",
    "conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DESKTOP-VBRKFH1\\SQLEXPRESS;'\n",
    "                      'Database=Bar;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "engine = sqlalchemy.create_engine(\"mssql+pyodbc://<username>:<password>@<dsnname>\",pool_pre_ping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder to extract the data\n",
    "analyze_folder = \"D://Documentos//Data Science//Personal Projects//Jobcity Challenge//Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining diferente functions to act over diferents dataframes\n",
    "\n",
    "def df_transform(df): #Changin formats and adding new columns to the original dataframe.\n",
    "    df[\"Origin_x\"] = df.origin_coord.str.split(expand=True)[1].str.replace(\"(\",\"\").astype(float).round(0)\n",
    "    df[\"Origin_y\"] = df.origin_coord.str.split(expand=True)[2].str.replace(\")\",\"\").astype(float).round(0)\n",
    "\n",
    "    df[\"Destination_x\"] = df.destination_coord.str.split(expand=True)[1].str.replace(\"(\",\"\").astype(float).round(0)\n",
    "    df[\"Destination_y\"] = df.destination_coord.str.split(expand=True)[2].str.replace(\")\",\"\").astype(float).round(0)\n",
    "\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    time_window = \"30min\"\n",
    "\n",
    "    df[\"Time\"] = df['datetime'].dt.round(time_window).dt.strftime('%H:%M')\n",
    "    df[\"Week\"] = df['datetime'].dt.isocalendar().week\n",
    "    \n",
    "    return df\n",
    "\n",
    "def creating_tables (df): #Creating two new dataframes \n",
    "\n",
    "    df_grouped = df.groupby([\"Origin_x\",\"Origin_y\",\"Destination_x\",\"Destination_y\",\"Time\"]).agg({\"region\": \"min\",\"origin_coord\": \"min\",\"destination_coord\" : \"min\",\"datetime\":\"min\"}).reset_index()\n",
    "    df_grouped_2 = df.groupby([\"Week\",\"region\"]).agg({\"origin_coord\":\"count\"}).sort_index().reset_index()\n",
    "    \n",
    "    return df_grouped, df_grouped_2\n",
    "\n",
    "def  inserting_sql(df,df2,df3): #Saving the dataframes in the SQL server.\n",
    "    \n",
    "    df.to_sql('trips', con = engine,if_exists = 'append', index = False)\n",
    "    df2.to_sql('trips_per_origin', con = engine ,if_exists = 'replace', index = False)\n",
    "    df3.to_sql('avg_trips_week', con = engine ,if_exists = 'replace', index = False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File trips.csv finished\n",
      "Process Finished!\n"
     ]
    }
   ],
   "source": [
    "#Creating a loop to load everyfile in the folder to SQL. \n",
    "\n",
    "for file in os.listdir(analyze_folder):\n",
    "    if file.split(\".\")[1] == \"csv\":    \n",
    "        analytics_df = pd.read_csv(analyze_folder + \"//\" + file)\n",
    "        analytics_df = df_transform(analytics_df)\n",
    "        df_grouped, df_grouped_2 = creating_tables(analytics_df)\n",
    "        inserting_sql(analytics_df,df_grouped,df_grouped_2)\n",
    "        \n",
    "        print (\"File \" + str(file) + \" finished\")\n",
    "        \n",
    "    elif file.split(\".\")[1] == \"xlsx\":    \n",
    "        analytics_df = pd.read_excel(analyze_folder + \"//\" + file)\n",
    "        analytics_df = df_transform(analytics_df)\n",
    "        df_grouped, df_grouped_2 = creating_tables(analytics_df)\n",
    "        inserting_sql(analytics_df,df_grouped,df_grouped_2)\n",
    "        \n",
    "        print (\"file\" + str(file) + \"finished\")\n",
    "\n",
    "print (\"Process Finished!\")\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
